{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Data Science\n",
    "\n",
    "How do we use data science in real world applications? \n",
    "\n",
    "*About the Data*\n",
    "\n",
    "We are using a dataset from Kaggle that gives us the position, salary, and year of each White House employee from 2009-2017. You can reade more about our source and how they obtained their data here: https://www.kaggle.com/adamschroeder/white-house-salaries \n",
    "\n",
    "*Our Procedure* \n",
    "\n",
    "This workshop will focus on conducting a simple permutation test on the salaries from the year 2016 and the year 2017. Our goal is to investigate the differences between the salaries of the last year Obama administration staff and the first year Trump administration staff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading in Our Libraries \n",
    "\n",
    "Today we will be using Pandas, matplotlib, Seaborn, regular expressions, and NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Loading in Our Data\n",
    "\n",
    "Since our CSV file is in the same folder as our notebook, we can read the CSV with a direct call to the file name instead of having to write a full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama = pd.read_csv(\"obama_staff_salaries.csv\")\n",
    "trump = pd.read_csv(\"white_house_2017_salaries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Cleaning and EDA\n",
    "\n",
    "First, we need to filter our Obama data to only consist of the year 2016. This is because this data set ranges from 2009-2016. Our Trump data is already only a year long, so that remains. However, we must standardize the data forms across both data frames so that we can permute them. This is because when we shuffle our data, we will want to find the test statistic for each shuffle we make. Thus, if the datatypes are different, there will be errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use logical filtering \n",
    "\n",
    "obama16 = ...\n",
    "\n",
    "# We will make the salary column into integers\n",
    "\n",
    "obama16.loc[:, \"salary\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's make a quick detour to get a more comprehensive look at our Obama data. Since we will only be comparing one year, 2016, let's use our filtering techniques above to see if 2016 was an outlier, or had an abnormal value. In this case, comparing 2016 to 2017 might not be the best permutation test without more information. Our goal is to compare the salary rate for both administrations. An outlier would be caused by many more cofounding variables that we would have to address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to strip the dollar sign character ('$') and convert\n",
    "# The data type into floats to do analysis on it\n",
    "\n",
    "obama.loc[:, \"salary\"] = ...\n",
    "\n",
    "# obama.loc[:, ['salary', 'year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Let's move on to cleaning the Trump data. We're going to use something called [regular expressions](https://docs.python.org/3/howto/regex.html) to filter and convert the data and then use list comprehension to loop through the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to test our regular expression on a single value in\n",
    "# Our Trump dataset by forcing an error if our regular expression doesn't work \n",
    "\n",
    "test_val = ...\n",
    "\n",
    "# assert re.match('\\$', 'hi')\n",
    "\n",
    "assert re.match('\\$', test_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to clean our Trump dollar data. We need a clean integer to do analysis. This is because our Obama data is also in a clean integer format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split, join, strip, float, int! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension format \n",
    "# [procedure(x) if (filter == True) else x for x in (list to iterate through)]\n",
    "\n",
    "trump.loc[:, \"SALARY\"] = ...\n",
    "trump['salary'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Hypothesis Testing\n",
    "\n",
    "How did salaries change from the Obama administration to the Trump administration? \n",
    "\n",
    "To set up our hypothesis, we need to determine the test statistic we want to use. A test statistic is an observed value that we will use later to calculate our p-value. \n",
    "\n",
    "A good statistic when we're working with salary data is the median because it isn't as affected by outliers. Instead, it gives us a value that we know has 50% of the data less than it and 50% of the data larger than it. Salary data is often misrepresented by the average because the distribution is rarely normal. We will look at histograms of our salary data to visualize this statement. \n",
    "\n",
    "Once we find our two sample medians, we can go ahead and define our test statistic. Our test statistic will be the difference between our observed median statistics. \n",
    "\n",
    "Let's define our parameters. Parameters are different than statistics because they represent the entire population, not just our sample. They are usually unknown. If we knew them, there would be no need for statistical procedures!\n",
    "\n",
    "$M_t$ = True median of Trump administration salaries \n",
    "\n",
    "$M_o$ = True median of Obama administration salaries\n",
    "\n",
    "We will assume both of our data sets are simple random samples from the universe of all possible Trump and Obama administration salaries. That is, the only difference in salary median we see is due by chance. \n",
    "\n",
    "So, we say:\n",
    "\n",
    "$H_o =  M_t - M_o = 0$ \n",
    "\n",
    "$H_a = M_t - M_o \\neq 0$\n",
    "\n",
    "$H_o$ is our null hypothesis, which says that the distribution for salaries is the same for the Obama administration and Trump administration and any variation is due by chance. \n",
    "\n",
    "$H_a$ is our alternative hypothesis, which says that the two distributions are not equal. We will be using a double-sided hypothesis, which means that the difference could be positive or negative on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to find the median with a simple series method\n",
    "\n",
    "# Below is our formatting for printing in Jupyter notebooks\n",
    "print(\"Trump salary median:\", m_t)\n",
    "print(\"Trump salary mean:\", ...)\n",
    "\n",
    "# Here is our histogram\n",
    "# sns.distplot(data, kde=False, bins=np.arange())\n",
    "sns.distplot(...)\n",
    "\n",
    "plt.axvline(m_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to find the median with a simple series method\n",
    "\n",
    "# Below is our formatting for printing in Jupyter notebooks\n",
    "print(\"Obama salary median:\", m_o)\n",
    "print(\"Obama salary mean:\", ...)\n",
    "\n",
    "sns.distplot(...)\n",
    "\n",
    "plt.axvline(m_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat = ...\n",
    "t_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A/B Testing \n",
    "\n",
    "To test our hypothesis, we will be using A/B testing. It is a variation of a permutation test. Under the null hypothesis, there should be no difference whether a salary was from the Obama administration or Trump administration on the median of each respective group. That's because the null hypothesis states that both years are simple random samples from the same distribution of salaries.\n",
    "\n",
    "To mimic this, we will \"shuffle\" the data and give new labels to all the salaries. \n",
    "\n",
    "Then, we will group by president and find the median for these \"new\" administration salaries. \n",
    "\n",
    "We will repeat that many, many times (thousands) to simulate what it would be like if we took every permutation possible. Why don't we just produce every permutation possible? That would be (472 + 377)! times ... A really big number! Much bigger than we need to get accurate results.\n",
    "\n",
    "At the end, we will make a histogram of all our permuted test statistics. It should appear roughly normal because of the central limit theorem. Because we are simulating the null distribution, it should be centered around 0 (if both years are, under the null, from the same distribution, than their true median is the same!). After we see our histogram, we will find the p-value.\n",
    "\n",
    "To start our permutation, we need to assign each row in each table an indicator that we can shuffle. We will simply use the names of the president in term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a series of each string and multiplying it by that length \n",
    "\n",
    "obama16['pres'] = ...\n",
    "trump['pres'] = ...\n",
    "\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making one big giant table \n",
    "\n",
    "# Only select the columns we need\n",
    "trump_pmtn = ...\n",
    "obama_pmtn = ...\n",
    "\n",
    "# Attaching the two tables using concat\n",
    "\n",
    "salaries = ...\n",
    "\n",
    "# Getting a Series to shuffle with that has 472 'obama's and 377 'trump's\n",
    "\n",
    "pres_arr = ...\n",
    "\n",
    "salaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here comes the fun part. To shuffle the data, we will create a for loop that will give us an array of test statistics, one for each loop. We will usually have this loop give us 5000 to 10000 statistics. This will simulate getting the test statistic for all the possible permutations of labels to this data. We will go through how we shuffle and obtain a new statistic, then generalize it for the for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's break down what we will be putting in our for loop\n",
    "\n",
    "test_salary = ...\n",
    "\n",
    "# Shuffle! \n",
    "\n",
    "test_salary # run this a few times and see what happens!\n",
    "\n",
    "# now make a new df \n",
    "\n",
    "pd.DataFrame(...)\n",
    "\n",
    "# group it like before & use method\n",
    "\n",
    "pd.DataFrame(...).groupby(...).median() \n",
    "\n",
    "# make new DF \n",
    "\n",
    "test_df = ...\n",
    "\n",
    "# what's the statistic? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array of how many repetitions we want\n",
    "rep = ...\n",
    "\n",
    "# create an empty array to store our data\n",
    "null_stats = []\n",
    "\n",
    "for i in rep: \n",
    "    # obtain the salaries like before and shuffle\n",
    "    \n",
    "    # assign to a new df\n",
    "    \n",
    "    # get the test statistic\n",
    "    \n",
    "    # place it in the empty array to collect them all \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: P-Value and Conclusions \n",
    "\n",
    "What's a p-value? It's the probability, under the null, that we see a value less than or equal to our *observed* test statistic. What does it mean to be a probability under the null? We have simulated what our null distribution looks like from our for loop above. Just like with a normal distribution, we now have probabilities associated with different values. \n",
    "\n",
    "When dealing with a normal distribution, he father your value gets away from the center of a distribution, the less likely you will find a value even greater than it. Similarly, calculating the p-value will give us an idea of how abnormal our observed test statistic is. \n",
    "\n",
    "Now we are going to take our array, which simulates the test statistic of all permutations under the null, and see how often we saw a test statistic greater than (or less than the negative of) our test statistic value to calculate our p-value. \n",
    "\n",
    "The p-value inherently tells us nothing. We must decide how significant our p-value is by choosing a significance level. We will use the classic 0.05 cutoff. So, if only 0.05 of our permutations under the null show a value greater than (or less than the negative of) our test statistic, we can reject the null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(...)\n",
    "\n",
    "# np.count_nonzero is our friend! \n",
    "\n",
    "pval = ...\n",
    "\n",
    "print(\"p-value:{0}\".format(...))\n",
    "\n",
    "# Visualize where our test stat lies\n",
    "\n",
    "plt.axvline(t_stat, color='r')\n",
    "plt.axvline(-t_stat, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, do we reject the null?\n",
    "\n",
    "Yes.\n",
    "\n",
    "What does this mean? \n",
    "\n",
    "Rejecting the null doesn't give us any claim of causation. We cannot say the changing administration had an effect on the salaries, even though that's the only variable we checked. This is because we treated our data as a simple random sample, not an experiment with controls. \n",
    "\n",
    "What could have produced these results?\n",
    "\n",
    "Next, we should probably look at the total of all the salaries. Maybe the spending is the same but there are fewer positions. We could conduct this same procedure on the sum to get our result, or take advantage of the central limit theorem on sums to perform a t-test. Additionally, we need to look at the economy and state of the administration as a whole. Or, we could use a simple random sample of the salaries from the entire Obama administration and permutate with that, and do it repeatedly. There are potentially thousands of cofounding factors we could check. For the sake of time and resources, however, it's difficult to prove causation for simple random samples. Instead, we will be satisfied with the information the p-value gives us. We are confident that this outcome was not due to chance. \n",
    "\n",
    "And that's the end of the Practical Data Science Workshop! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
